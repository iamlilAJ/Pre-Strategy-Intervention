# "TOTAL_TIMESTEPS": 1e10

"TOTAL_TIMESTEPS": 1e9
"NUM_ENVS": 1024
"NUM_STEPS": 1
"HIDDEN_SIZE": 512
"NUM_LAYERS": 3
"NORM_TYPE": "layer_norm"
"NORM_INPUTS": False
"DUELING": True
"EPS_START": 0.01
"EPS_FINISH": 0.001
"EPS_DECAY": 0.1
"MAX_GRAD_NORM": 0.5
"NUM_MINIBATCHES": 1
"NUM_EPOCHS": 1
"LR": 3e-4
"LR_LINEAR_DECAY": False
"LAMBDA": 0.
"GAMMA": 0.99
"MEMORY_WINDOW": 0

"SEED": 0



# evaluate
"TEST_DURING_TRAINING": True
"TEST_INTERVAL": 0.01 # as a fraction of updates, i.e. log every 1% of training process
"TEST_NUM_STEPS": 100
"TEST_NUM_ENVS": 3000 # number of episodes to average over, can affect performance

#"ALG_NAME": "pqn_vdn_ff" # if you want to change the name of the algo in the metrics
"NUM_PROXY_AGENTS": 1
"OBS_ENC_HIDDEN_DIM": 64
"TEMPERATURE": 1.0
"NODE_FEATURE_DIM": 8



"ENV_NAME": "hanabi"
"ENV_KWARGS": {
  "num_agents": 2,
  "train_pre_policy": True,
  "intrinsic_reward_ratio": 0.02,
  "if_augment_obs": False,
  "convention_type": "save_5"
}



"RUN_BASELINE": True
"ALG_NAME": "intrinsic_reward_PQN"
